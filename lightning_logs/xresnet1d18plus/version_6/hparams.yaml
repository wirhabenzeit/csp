Optimizer: !!python/name:torch.optim.adam.Adam ''
accumulate_grad_batches: 1
batch_size: 64
center_csp: true
drift_mag: 0.3
epochs: 1000
gamma: 0.9931160484209338
lr: 0.01
lr_end: 1.0e-05
momentum: 0.5
noise_mag: 0.0
scale_mag: 3.0
shift_mag: 0.2
swa_epoch_start: 0.8
test_copies: 5
window_size: 256
