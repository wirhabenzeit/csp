Optimizer: !!python/name:torch.optim.adam.Adam ''
Scheduler: !!python/name:torch.optim.lr_scheduler.OneCycleLR ''
accumulate_grad_batches: 1
batch_size: 50
center_csp: false
drift_mag: 0.1
epochs: 5000
lr: 0.01
noise_mag: 1.0e-05
optimizer_kwargs:
  eps: 1.0e-08
scale_mag: 3.0
scheduler_kwargs:
  epochs: 5000
  max_lr: 0.01
  steps_per_epoch: 1
  three_phase: false
shift_mag: 0.2
swa_epoch_start: 0.8
swa_lrs: 0.0001
test_copies: 5
window_size: 512
